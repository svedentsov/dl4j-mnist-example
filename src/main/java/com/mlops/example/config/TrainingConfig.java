package com.mlops.example.config;

import com.fasterxml.jackson.annotation.JsonIgnore;
import lombok.Builder;
import lombok.Getter;
import lombok.ToString;

import java.nio.file.Path;
import java.nio.file.Paths;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;

/**
 * Класс-конфигурация, инкапсулирующий все параметры для процесса обучения.
 * Использует шаблон Builder для удобного и безопасного создания immutable-экземпляров.
 * <p>
 * Этот объект является "единственным источником правды" (single source of truth) для
 * конкретного запуска обучения. Сохранение этой конфигурации вместе с обученной моделью
 * является ключевым аспектом для обеспечения **воспроизводимости** и **отслеживаемости**
 * экспериментов в MLOps-цикле.
 */
@Getter
@Builder
@ToString
public class TrainingConfig {

    /**
     * Количество эпох. Одна эпоха — это один полный проход всего тренировочного набора данных
     * через нейронную сеть.
     * <p>
     * <b>Влияние:</b>
     * <ul>
     *     <li><b>Слишком мало эпох:</b> Модель может не успеть обучиться и будет показывать низкое качество (недообучение, underfitting).</li>
     *     <li><b>Слишком много эпох:</b> Модель может "запомнить" тренировочные данные и плохо работать на новых, невиданных данных (переобучение, overfitting). Также это приводит к лишним затратам вычислительных ресурсов.</li>
     * </ul>
     */
    private final int epochs;

    /**
     * Размер батча (пакета). Это количество обучающих примеров, которые обрабатываются за одну итерацию
     * перед обновлением весов модели.
     * <p>
     * <b>Влияние:</b>
     * <ul>
     *     <li><b>Маленький размер:</b> Обновления весов более "шумные", что иногда помогает модели найти лучший минимум. Требует меньше памяти (VRAM), но общее время обучения может увеличиться из-за большего числа итераций.</li>
     *     <li><b>Большой размер:</b> Обновления весов более стабильные. Обучение может быть быстрее за счет эффективной векторизации на GPU. Однако требует больше памяти и может привести к тому, что модель "застрянет" в менее оптимальном решении.</li>
     * </ul>
     */
    private final int batchSize;

    /**
     * Скорость обучения (Learning Rate). Один из самых важных гиперпараметров. Определяет размер шага,
     * который делает алгоритм оптимизации (в нашем случае Adam) при обновлении весов.
     * <p>
     * <b>Влияние:</b>
     * <ul>
     *     <li><b>Слишком высокая:</b> Оптимизатор может "перепрыгивать" через оптимальное решение, и модель не сойдется.</li>
     *     <li><b>Слишком низкая:</b> Обучение будет идти очень медленно и может застрять в локальном минимуме.</li>
     * </ul>
     */
    private final double learningRate;

    /**
     * Начальное значение (seed) для генератора случайных чисел. Используется для инициализации весов
     * модели и других случайных процессов (например, перемешивания данных).
     * <p>
     * <b>Значение для QA и MLOps:</b> Это критически важный параметр для **воспроизводимости**.
     * При одинаковом seed, коде и данных два запуска обучения дадут абсолютно идентичные результаты,
     * что позволяет проводить надежное регрессионное тестирование моделей.
     */
    private final int randomSeed;

    // Параметры ввода/вывода

    /**
     * Базовая директория для сохранения всех артефактов обучения (модель, метаданные, конфигурация).
     * Использование отдельной директории помогает поддерживать порядок в проекте.
     */
    private final Path baseOutputDir;

    /**
     * Флаг, указывающий, нужно ли сохранять состояние оптимизатора (updater'а) вместе с моделью.
     * <p>
     * <b>Применение:</b>
     * <ul>
     *     <li><b>true:</b> Необходимо, если вы планируете **возобновлять обучение** (fine-tuning) модели с того места, где остановились. Оптимизаторы типа Adam имеют внутреннее состояние (например, накопленные градиенты), которое важно сохранить.</li>
     *     <li><b>false:</b> Можно использовать, если модель будет использоваться только для предсказаний (inference) и ее дообучение не планируется. Экономит место на диске.</li>
     * </ul>
     */
    private final boolean saveUpdater;

    /**
     * Возвращает уникальный путь к директории для конкретного запуска обучения.
     * Создание уникальной папки с временной меткой для каждого запуска — это лучшая практика,
     * которая предотвращает перезапись результатов и позволяет легко сравнивать разные эксперименты.
     * <p>
     * Формат: {@code {baseOutputDir}/train_{YYYY-MM-dd_HH-mm-ss}}
     *
     * @return Path к уникальной директории вывода.
     */
    @JsonIgnore // Не сериализуем этот вычисляемый путь в JSON
    public Path getRunOutputDir() {
        String timestamp = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyy-MM-dd_HH-mm-ss"));
        return baseOutputDir.resolve("train_" + timestamp);
    }

    /**
     * Создает конфигурацию с параметрами по умолчанию.
     * Предоставляет удобную и консистентную базовую конфигурацию для быстрых тестов,
     * демонстраций или для пользователей, которые не хотят указывать все параметры вручную.
     *
     * @return Экземпляр TrainingConfig со значениями по умолчанию.
     */
    public static TrainingConfig defaultConfig() {
        return TrainingConfig.builder()
                .epochs(2)
                .batchSize(64)
                .learningRate(1e-3)
                .randomSeed(12345)
                .baseOutputDir(Paths.get(".")) // Сохраняем в корень проекта по умолчанию
                .saveUpdater(true)
                .build();
    }
}
